# Introduction {#sec:introduction}

\pagenumbering{arabic}
<!--- Use \shorthandoff{"} for german documents -->

<!---
You can use inline comments to organize yourself
-->

<!--- 
- [x] Start using boilerplate
- [ ] Your next TODO
-->

Lorem ipsum...

## Motivation

<!--- PARAGRAPH 1 - DESCRIBE IT HERE -->

<!-- Also find inspiration for the intro here http://www.diva-portal.org/smash/get/diva2:24228/FULLTEXT01.pdf -->

\todo{Introduce into the topic of event stores (why/what) and replication (why/what)}

Distributed event-based systems are the key to increase the scalability of today’s information systems [@buchmann2009complex].

At the foundation of data-intense applications are distributed event stores...

TODO edge computing (including reference) [@cao2020overview]
While event stores embedded on IoT devices etc. must be ultra-lightweight and fast, when deployed to the cloud, they must provide reliability characteristics...

there are a lot of requirements to today's distributed systems: They must be able to ingest high throughputs, store a massive load of data (what is a massive load?) and respond almost immediately to user queries, while providing fault-tolerance, consistency and high availability at the same time.
 
 need to meet these requirements while serving millions of users...
 
For distributed event stores, those requirements are even higher (why?). With modern approaches to replication, it is possible to meet this requirements for both vertical and horizontal scalability...

Various replication algorithms have been proposed to achieve availability under different consistency and fault-tolerance conditions. This work tries to identify such a replication protocol that has the desired performance characteristics for ChronicleDB, a high-throughput event store.

\todo{Better, catchy introduction. See other work for inspiration}

<!--

TODO describe todays demand for fast, distributed, scalable event stores

TODO describe dist sys

Computer systems are intrinsically complex: a single computer operates resting on the interaction of multiple hardware and software components, each of which can fail for a variety of reasons (power failures, human errors,
etc.). Such complexity vastly amplifies for distributed systems, which require multiple computer nodes to interact
remotely.
In spite of their considerable complexity, distributed systems have become fundamental to many current application
domains, such as industrial control, infrastructure management, and internet banking, just to name a few. All of them
are subject to requirements of availability, integrity and robustness in the face of failures. In other words, they have to be
fault-tolerant.
A system is said to be fault-tolerant if it is able to react gracefully and in a planned manner to any fault that may occur
by either entering a well-defined alternative behavior or resiliently continuing operation in the face of the fault.1
Redundancy is key to achieving the degree of robustness needed to be able to mask faults, wholly or partially,
to users. One common way to attain redundancy is by running replicated state machines on multiple nodes of the
system.2

The software systems providing our services are expected to dynamically auto-scale at runtime in correlation with the workload. In support of this shift, the monolithic model of building applications is rapidly giving way to the model of building micro-services that are clustered and deployed in a highly distributed manner [@newman2021building]. 

TODO describe event stores

The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis

At the origin of this work is the wish to investigate whether the use of the Raft consensus algorithm might fit the
domain of event stores, to make a standalone event store distributed...

List event store Use cases briefly: For example, processing user data from saas 
"Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value"  https://dl.acm.org/doi/abs/10.1145/2463676.2465272

-->

<!--
General motivation for your work, context and goals: 1-2 pages
Make sure to address the following: 
• Context: make sure to link where your work fits in
• Problem: gap in knowledge, too expensive, too slow, a deficiency, superseded technology
• Strategy: the way you will address the problem
-->

## Problem Formulation

\todo{Introduce into research methodology and questions/hypotheses}

<!--
1.2 Problem Formulation
This brings us to the subject of this thesis. We study reconciliation algorithms for systems with eventually strict consistency requirements. These
systems can accept consistency to be temporarily violated during a network
partition, but require that the system is fully consistent once the system is
reconciled. Specifically, we are interested in systems where consistency can
be expressed using data integrity constraints as in the car example above.
Our main hypothesis is that network partitions can be effectively tolerated by data-centric applications by using an optimistic approach and to
reconcile conflicts afterwards. Moreover, we theorise that this can be done
with the help of a general purpose middleware. To support this claim we
must explore the possible solution space and answer a number of research
questions:
• Does acting optimistically during network partitions pay off, even in
presence of integrity constraints?
• Which is preferable, state or operation based reconciliation?
• What can be done to optimise operation based reconciliation?
• Is it possible and/or worthwhile to serve new incoming operations
during reconciliation?
• Can such support be integrated as part of a general middleware?
Although this thesis concentrates on the reconciliation part of a partitiontolerant middleware, the work is part of a larger context. In the European
DeDiSys project the goal is to create partition-tolerant middleware to increase the availability for applications.
-->

This brings us to the subject of this thesis. We try to find and evaluate an algorithm that has the desirable performance properties for an event store with defined requirements. To find such an algorithm, we must explore the possible solution space and answer a number of research questions:

- Explorative Research: No strong first hypothesis; a practical, qualitative approach; the goals are

- Identification of a consistency model suitable for an event store with a specific set of requirements and differentiation from other consistency models including a detailed discussion of the advantages and disadvantages of the models in question.
- Identification and justification of a replication protocol suitable for an event store that satisfies the characteristic requirements of the selected consistency model, including a differentiation from other replication techniques
- Investigation of the performance trade-off of an implementation of the chosen replication protocol in an event store to make it fault-tolerant and horizontally scalable, compared with the expected trade-off (quantitative approach).

- One goal: find a plausible grounded theory about making a event store fault-tolerant and horizontally scalable by replication, to build new hypotheses and opportunities on.
- Get an overview and rough insight into the (unexplored) subject area of replication for event stores, then test the hypothesis on performance

- TODO List (implicitly?) and answer (explicitly if possible or keep open for the thesis) the research question(s)
    - What are the unique characteristics of event stores? 
    - What are the use cases of event stores?
    - Which role do event stores play in distributed systems and what are the requirements on them in this context?
    - How to make event stores fault tolerant?
        - .. we want ... to be still operational... when one node fails...
    - What is replication, and why do we need it (to make it fault tolerant)?
        - Why is there particular interest in researching this for event stores?
    - What are the different replication protocols and what are the differences between them?
    - Which protocol fits best and why? What are the advantages of this protocol and which are the disadvantages? 
        - What is the performance/throughput impact and what influences it? What are ways to improve it?
            - Are there any upper limits?
        - What do users have to do without?

- TODO Write a clear, interesting and specific hypothesis
    - With the utilization of a replication protocol, an event store becomes horizontally scalable, i.e., highly available, fault-tolerant, and more performant (as the number of streams increases).

- TODO thesis statement

See https://nmbu.instructure.com/courses/2280/pages/research-questions-hypotheses-and-thesis-statements?module_item_id=15684

<!--
The contributions of this work are as follows:
• A fresh open-source implementation of Raft using the Go programming language, which can be used as a learning
base or a building block for multiple other uses, professional, educational, inspirational.
• The performance testing of two Proof-of-Concept (PoC) game implementations with different requirements (real-time
vs. fully turn-based) against two alternative Raft network architectures.
• The open-source implementation of the Raft extension described in Reference 6, which addresses Byzantine behavior.
• A simple Node.js based tool called raft_analyzer, which can be used to aggregate and analyse Raft log traces to verify
their adherence to Raft’s safety properties (election safety, leader append-only, log matching, leader completeness, state
machine safety). This tool also provides a web interface for the graphical display of all the coordination communication
flowing within the cluster.
• Empirical evidence that hints quantifiably at the performance penalty of using replication to achieve robustness. Our
results show how our two architectures scale, by measuring simple metrics such as the delay between user actions and
game feedback.
• Our code is available in open source in the public domain, at https://github.com/cornacchia/go-raft-multiplayer-poc
-->

## Contribution

To the best knowledge, this work is the first attempt published in academia focusing on applying the Raft consensus protocol to event stores. There are a few event stores and time series databases used in industry that leverage Raft to achieve fault-tolerance and scalability (InfluxDB, IoTDB), but replication and consensus where only mentioned as a side note in academic research on those systems. 

In this thesis we discuss and analyse several different replication algorithms to find the one that fits our requirements for an event store. The contributions are:

* A discussion of...

\todo{This bullet point}

* An implementation of a replicated ChronicleDB event store based on Apache Ratis [@konrad2022chroniclecloud], to serve as a learning base for evaluating the consistency model and replication protocol that we found most useful. The code is available in open source in the public domain, at https://github.com/christian-konrad/raft-log-replication-demo

\todo{Update repo link(s)}

* Benchmark-based performance evaluations of the implementation on event-store-specific metrics (event throughput, query speed) to study the throughput and scalability of network architectures with different numbers of nodes.

<!--- PARAGRAPH 2 - DESCRIBE YOUR STRUCTURE HERE -->

## Outline

The remainder of this work is structured as follows: [Chapter 1](#sec:background) introduces the reader to the research context of this work, examines the Raft Consensus Protocol and the ChronicleDB event store, and discusses recent literature in this area.
[Chapter 2](#sec:implementation) describes the methodology underlying this research, presents the main implementation choices and compares recent work. [Chapter 3](#sec:evaluation) then illustrates the results of the evaluation of the implementation. [Chapter 4](#sec:conclusion) finally draws conclusions from this work and outlines recommendations, key learnings, weaknesses of this approach and future challenges.
