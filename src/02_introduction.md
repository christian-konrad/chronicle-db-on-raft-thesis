# Introduction {#sec:introduction}

\pagenumbering{arabic}
<!--- Use \shorthandoff{"} for german documents -->

<!--
General motivation for your work, context and goals: 1-2 pages
Make sure to address the following: 
• Context: make sure to link where your work fits in
• Problem: gap in knowledge, too expensive, too slow, a deficiency, superseded technology
• Strategy: the way you will address the problem
-->

Distributed event-based systems are the key to increase the scalability of today’s information systems [@buchmann2009complex]. At the foundation of data-intense applications are distributed event stores that stretch beyond devices in the cloud and locally to form _edge-cloud networks_ [@cao2020overview]. Both research and enterprise applications today require systems that provide low latency, high availability and realtime monitoring capabilities, many of them powered by the _Internet of Things_ (IoT) and _Complex Event Processing_ (CEP [@buchmann2009complex]), such as industrial big data and smart factories [@wang2018industrial; @mourtzis2016industrial], high-bandwidth clinical and health applications including sensor data from smart wearables [@smuck2021emerging], self-driving cars, smart cities, smart energy grids [@liu2015real], or environments supporting scientific experiments [@elias2022real; @holzner2015high] (e.g. at the CERN large hadron collider) to name a few. Typically, those systems consume a huge amount of events generated by millions of users, sensors and devices. The rise of 5G networks drives innovation in this area and therefore increases the need for those systems to seize this opportunities in full extend [@hassan2019edge].

## Motivation

<!-- Introduce into the context -->
<!-- DONE -->

Due to these increasing demands, such event-based systems must therefore be able to write events arriving at very high and fluctuating rates to persistent storage and support ad hoc analytical queries. Since conventional database systems are not able to provide the required write performance, novel write-optimized data stores such as log-based systems or key-value stores have been introduced recently. However, the drawbacks of these systems are mediocre query performance and the lack of appropriate mechanisms for immediate recovery in case of system failures. 
As a result, event stores emerged, such as ChronicleDB, a novel high-throughput database system with a storage layout tailored to provide high write performance for fluctuating data volumes and powerful indexing capabilities to support a wide range of queries. ChronicleDB has been shown to outperform competing systems in both write and read performance [@seidemann2019chronicledb]. At present, ChronicleDB is designed either as an embeddable library for tight integration into an application or as a standalone database server, but without the ability to run in a scalable cluster architecture.

While event stores running locally or embedded on IoT devices must be ultra-lightweight and fast, when deployed to the cloud, they must provide strong reliability characteristics. Furthermore, a smooth interplay between this different types of deployments is crucial: the cloud-deployed store must be able to reliably ingest a huge amount of different streams, with both different or equal event schemas, that are fed into it by the embedded systems.  

With edge computing, the requirements on today's distributed systems increase significantly: they must be able to maintain high throughput, store a huge amount of data efficiently, and respond almost immediately to user queries, while providing the desired level of fault-tolerance, consistency, and high availability. All of these requirements must be met while serving millions of users at the same time.

Approaching a solution to this problems leads to _data replication_. Replication of data resources both within a local computing cluster and across different geographic regions ensures availability and consistency to a certain degree in the event of a node failure resulting from various types of faults. An effective replication protocol not only enables efficient access to vast amounts of data, but also reduces latency in large-scale, geo-replicated cloud environments.

Fortunately, several replication algorithms have already been proposed to achieve availability under different consistency and fault-tolerance conditions. This work attempts to identify such a replication protocol that meets the desired write and query performance characteristics for ChronicleDB, and then find an implementation design so that it can be used as a fault-tolerant, highly available, and scalable cluster in addition to the embedded and standalone server modes.



## Problem Formulation

<!-- Introduce into research methodology and questions/hypotheses -->
<!-- DONE -->

This brings us to the subject of this thesis. We try to find and evaluate an algorithm that has the desirable performance properties for an event store with defined dependability requirements. To find such an algorithm, we use an explorative research approach including both quantitative and qualitative methods to discover the possible solution space:

- Identification of a consistency model suitable for an event store with a specific set of dependability requirements and differentiation from other consistency models including a detailed discussion of the advantages and disadvantages of the models in question.
- Identification and justification of a replication protocol suitable for an event store that satisfies the characteristic requirements of the selected consistency model, including a differentiation from other replication techniques.
- Investigation and quantitative analysis of the performance trade-off of an implementation of the chosen replication protocol in an event store to meet the given dependability requirements, including fault-tolerance and horizontal scalability, compared with the expected trade-off, as well as an examination of the dependability properties themselves.

Additionally, the results of this work serve as a plausible grounded theory about providing such a replication layer, to build new hypotheses and opportunities on. Therefore, we answer the following research questions:

- What are the unique characteristics of event stores and how do they influence the needs and requirements for a replication layer to make them fault-tolerant? 
- What are the advantages and disadvantages of the different replication protocols for the ChronicleDB event store?
- What is the performance and throughput impact and what influences it? What are ways to improve it?
- Why is there a special interest in this research?

Positive findings from this research work would provide worthwhile benefits to distributed event stores:

- The impact of the application of different consistency models and replication protocols to event stores is known and described and helps both academics and developers to decide which protocol to go for, depending on their use cases and requirements.

- Users can operate event stores in an edge-cloud architecture, leveraging the different characteristics of embedded, single-node vs. replicated, multi-node clusters in different places of their system design, to be able to deal with the high throughputs occuring in heavily distributed IoT systems.

- Open questions and known problems are identified and documented properly so that the performance can be further improved to meet the requirements of such event store architectures used in production.  

## Contribution

<!-- What are the contributions of this work? -->

To the best knowledge, this work is the first attempt published in academia focusing on applying the Raft consensus protocol to event stores. There are a few event stores and time series databases used in industry that leverage Raft to achieve fault-tolerance and scalability (InfluxDB, IoTDB), but replication and consensus where only mentioned as a side note in academic research on those systems. 

In this thesis we discuss and analyse several different replication algorithms to find the one that fits our requirements for an event store. The contributions are:

- A thorough discussion of consistency models and replication protocols for an edge-cloud ready event store with the capability to ingest very high throughputs. A state-of-the-art replication protocol is then selected to handle this requirements in a future-proof way.

- A systematic review of previous implementations of replication protocols, focusing on decisions related to consistency, dependability, levels of data replicated, and the replication protocols chosen.

- An implementation of a replicated ChronicleDB event store based on Apache Ratis [@konrad2022chroniclecloud], to serve as a learning base for evaluating the consistency model and replication protocol that we found most useful. The code is available in open source in the public domain, at https://github.com/christian-konrad/raft-log-replication-demo.

\todo{Update repo link(s)}

- Benchmark-based performance evaluations of the implementation on event-store-specific metrics (event throughput, query speed) to study the throughput and scalability of network architectures with different numbers of nodes.

## Outline

<!--- Describe your thesis structure here -->
<!-- DONE -->

The remainder of this work is structured as follows: [Chapter 1](#sec:background) introduces the reader to the research context of this work, examines the Raft Consensus Protocol and the ChronicleDB event store, and discusses recent literature in this area.
[Chapter 2](#sec:implementation) describes the methodology underlying this research, presents the main implementation choices and compares recent work. [Chapter 3](#sec:evaluation) then illustrates the results of the evaluation of the implementation. [Chapter 4](#sec:conclusion) finally draws conclusions from this work and outlines recommendations, key learnings, weaknesses of this approach and future challenges.
