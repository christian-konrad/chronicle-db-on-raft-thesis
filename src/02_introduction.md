# Introduction {#sec:introduction}

\pagenumbering{arabic}
<!--- Use \shorthandoff{"} for german documents -->

<!---
You can use inline comments to organize yourself
-->

<!--- 
- [x] Start using boilerplate
- [ ] Your next TODO
-->

Lorem ipsum...

## Motivation

<!--- PARAGRAPH 1 - DESCRIBE IT HERE -->

<!-- Also find inspiration for the intro here http://www.diva-portal.org/smash/get/diva2:24228/FULLTEXT01.pdf -->

\todo{Introduce into the topic of event stores (why/what) and replication (why/what)}

Distributed event-based systems are the key to increase the scalability of today’s information systems [@buchmann2009complex]. At the foundation of data-intense applications are distributed event stores that stretch beyond devices in the cloud and locally to form edge-cloud networks [@cao2020overview]. Both research and enterprises today require systems that provide low latency, high availability and realtime monitoring capabilities, such as manufacturing lines, [TODO other examples here like health applications/sensor data], to name a few. Typically, those systems consume a huge amount of events generated by millions of users, sensors and devices. The rise of 5G networks drives innovation in this area and therefore increases the need for those systems to seize this opportunities in full extend. 

TODO synonym for "huge amount" 

Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value"  https://dl.acm.org/doi/abs/10.1145/2463676.2465272

\todo{Citation on need of edge computing in enterprises}
\todo{Citation on rise of 5G}

While event stores running locally or embedded on IoT devices must be ultra-lightweight and fast, when deployed to the cloud, they must provide strong reliability characteristics. Furthermore, a smooth interplay between this different types of deployments is crucial: the cloud-deployed store must be able to reliably ingest a huge amount of different streams, with both different or equal event schemas, that are fed into it by the embedded systems.  

\todo{Better phrasing: What is massive?}
With edge computing, the requirements to today's distributed systems are increasing a lot: They must be able to ingest high throughputs, store a massive load of data efficiently and respond almost immediately to user queries, while providing desired levels of fault-tolerance, consistency and high availability at the same time. All this requirements must be met while serving millions of users simultaneously.

\todo{Re-read and maybe rephrase}

Approaching a solution to this problems leads to _data replication_. By replicating data resources both inside of a local computing cluster and across different geographic regions, availability and consistency are ensured to a certain degree in the case of a failure of a node resulting from different types of faults. An effective replication protocol does not only provide efficient access to massive amounts of data, it also reduces latency in large-scale, geo-replicated cloud environments.

Fortunatelly, various replication algorithms have already been proposed to achieve availability under different consistency and fault-tolerance conditions. This work tries to identify such a replication protocol that has the desired performance characteristics for ChronicleDB, a high-throughput event store.

<!--

TODO also include the following into above intro

Computer systems are intrinsically complex: a single computer operates resting on the interaction of multiple hardware and software components, each of which can fail for a variety of reasons (power failures, human errors,
etc.). Such complexity vastly amplifies for distributed systems, which require multiple computer nodes to interact remotely.
In spite of their considerable complexity, distributed systems have become fundamental to many current application
domains, such as industrial control, infrastructure management, and internet banking, just to name a few. All of them
are subject to requirements of availability, integrity and robustness in the face of failures. In other words, they have to be
fault-tolerant.
A system is said to be fault-tolerant if it is able to react gracefully and in a planned manner to any fault that may occur
by either entering a well-defined alternative behavior or resiliently continuing operation in the face of the fault.1
Redundancy is key to achieving the degree of robustness needed to be able to mask faults, wholly or partially,
to users. One common way to attain redundancy is by running replicated state machines on multiple nodes of the
system.2

The software systems providing our services are expected to dynamically auto-scale at runtime in correlation with the workload. In support of this shift, the monolithic model of building applications is rapidly giving way to the model of building micro-services that are clustered and deployed in a highly distributed manner [@newman2021building]. 

TODO describe event stores

The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis

At the origin of this work is the wish to investigate whether the use of the Raft consensus algorithm might fit the
domain of event stores, to make a standalone event store distributed...

List event store Use cases briefly: For example, processing user data from saas 
"Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value"  https://dl.acm.org/doi/abs/10.1145/2463676.2465272

-->

Unfortunetally, this topic is not covered well by research in the context of event stores...

<!--
General motivation for your work, context and goals: 1-2 pages
Make sure to address the following: 
• Context: make sure to link where your work fits in
• Problem: gap in knowledge, too expensive, too slow, a deficiency, superseded technology
• Strategy: the way you will address the problem
-->

## Problem Formulation

<!-- Introduce into research methodology and questions/hypotheses -->
<!-- DONE -->

This brings us to the subject of this thesis. We try to find and evaluate an algorithm that has the desirable performance properties for an event store with defined requirements. To find such an algorithm, we use an explorative research approach including both quantitative and qualitative methods to discover the possible solution space:

- Identification of a consistency model suitable for an event store with a specific set of requirements and differentiation from other consistency models including a detailed discussion of the advantages and disadvantages of the models in question.
- Identification and justification of a replication protocol suitable for an event store that satisfies the characteristic requirements of the selected consistency model, including a differentiation from other replication techniques.
- Investigation and quantitative analysis of the performance trade-off of an implementation of the chosen replication protocol in an event store to make it fault-tolerant and horizontally scalable, compared with the expected trade-off.

Additionally, the results of this work serve as a plausible grounded theory about providing such a replication layer, to build new hypotheses and opportunities on. Therefore, we answer the following research questions:

- What are the unique characteristics of event stores and how do they influence the needs and requirements for a replication layer to make them fault-tolerant? 
- What are the advantages and disadvantages of the different replication protocols for the ChronicleDB event store?
- What is the performance and throughput impact and what influences it? What are ways to improve it?
- Why is there a special interest in this research?

Positive findings from this research work would provide worthwhile benefits to distributed event stores:

- The impact of the application of different consistency models and replication protocols to event stores is known and described and helps both academics and developers to decide which protocol to go for, depending on their use cases and requirements.

- Users can operate event stores in an edge-cloud architecture, leveraging the different characteristics of embedded, single-node vs. replicated, multi-node clusters in different places of their system design, to be able to deal with the high throughputs occuring in heavily distributed IoT systems.

- Open questions and known problems are identified and documented properly so that the performance can be further improved to meet the requirements of such event store architectures used in production.  

## Contribution

<!-- What are the contributions of this work? -->

To the best knowledge, this work is the first attempt published in academia focusing on applying the Raft consensus protocol to event stores. There are a few event stores and time series databases used in industry that leverage Raft to achieve fault-tolerance and scalability (InfluxDB, IoTDB), but replication and consensus where only mentioned as a side note in academic research on those systems. 

In this thesis we discuss and analyse several different replication algorithms to find the one that fits our requirements for an event store. The contributions are:

- A thorough discussion of consistency models and replication protocols for an edge-cloud ready event store with the capability to ingest very high throughputs. A state-of-the-art replication protocol is then selected to handle this requirements in a future-proof way.

- An implementation of a replicated ChronicleDB event store based on Apache Ratis [@konrad2022chroniclecloud], to serve as a learning base for evaluating the consistency model and replication protocol that we found most useful. The code is available in open source in the public domain, at https://github.com/christian-konrad/raft-log-replication-demo.

\todo{Update repo link(s)}

- Benchmark-based performance evaluations of the implementation on event-store-specific metrics (event throughput, query speed) to study the throughput and scalability of network architectures with different numbers of nodes.

## Outline

<!--- Describe your thesis structure here -->
<!-- DONE -->

The remainder of this work is structured as follows: [Chapter 1](#sec:background) introduces the reader to the research context of this work, examines the Raft Consensus Protocol and the ChronicleDB event store, and discusses recent literature in this area.
[Chapter 2](#sec:implementation) describes the methodology underlying this research, presents the main implementation choices and compares recent work. [Chapter 3](#sec:evaluation) then illustrates the results of the evaluation of the implementation. [Chapter 4](#sec:conclusion) finally draws conclusions from this work and outlines recommendations, key learnings, weaknesses of this approach and future challenges.
